{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"smartphones_data.csv\")\n",
    "df1 = df.copy()\n",
    "print(\"Shape of dataset:\", df1.shape)\n",
    "\n",
    "# Data exploration\n",
    "df1.describe()\n",
    "print(\"Missing values:\\n\", df1.isnull().sum())\n",
    "\n",
    "# Drop duplicates\n",
    "df1.drop_duplicates(inplace=True)\n",
    "print(\"Shape after removing duplicates:\", df1.shape)\n",
    "\n",
    "# Fix column names\n",
    "print(\"Original columns:\", df1.columns)\n",
    "df1.rename(columns={'primery_rear_camera': 'primary_rear_camera',\n",
    "                     'primery_front_camera': 'primary_front_camera'}, inplace=True)\n",
    "print(\"Fixed columns:\", df1.columns)\n",
    "\n",
    "# Calculate missing percentage\n",
    "missing_percentage = (df1.isnull().sum() / len(df1)) * 100\n",
    "print(\"Missing percentage:\\n\", missing_percentage)\n",
    "\n",
    "# Fill missing values\n",
    "df1['has_fingerprints'].fillna(df1['has_fingerprints'].mode()[0], inplace=True)\n",
    "df1['has_nfc'].fillna(df1['has_nfc'].mode()[0], inplace=True)\n",
    "df1['has_5g'].fillna(df1['has_5g'].mode()[0], inplace=True)\n",
    "df1['num_core'].fillna(df1['num_core'].median(), inplace=True)\n",
    "df1['refresh_rate(hz)'].fillna(df1['refresh_rate(hz)'].median(), inplace=True)\n",
    "\n",
    "# Print unique values of categorical features\n",
    "print(\"Unique brand names:\", df1['brand_name'].unique())  \n",
    "print(\"Unique OS:\", df1['OS'].unique())  \n",
    "print(\"Unique processor brands:\", df1['processor_brand'].unique())  \n",
    "print(\"Unique display types:\", df1['display_types'].unique())\n",
    "\n",
    "# Standardize categorical values\n",
    "df1['brand_name'] = df1['brand_name'].replace({'moto': 'motorola', 'Other': 'other'})\n",
    "df1['processor_brand'] = df1['processor_brand'].replace({'tru-mediatek': 'mediatek',\n",
    "                                                         'huawei': 'hisilicon',\n",
    "                                                         'quad': 'other',\n",
    "                                                         'spreadtrum': 'unisoc',\n",
    "                                                         'st-ericsson': 'other'})\n",
    "\n",
    "# Visualize distribution of brands\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.countplot(data=df1, x='brand_name', color='blue')\n",
    "plt.title(\"Number of Smartphones by Brand\")\n",
    "plt.xlabel(\"Brand\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=50)\n",
    "plt.show()\n",
    "\n",
    "# Check skewness of numeric columns\n",
    "numeric_cols = df1.select_dtypes(include=['number'])\n",
    "skewness_values = numeric_cols.apply(skew)\n",
    "print(\"Skewness values:\\n\", skewness_values)\n",
    "\n",
    "# Define numeric columns for analysis\n",
    "numeric_cols = ['Price', 'RAM', 'storage', 'Battery_cap', 'num_core',\n",
    "                'primary_rear_camera', 'Num_Rear_Cameras', 'primary_front_camera',\n",
    "                'num_front_camera', 'display_size(inch)', 'refresh_rate(hz)']\n",
    "\n",
    "# Create boxplots for numeric columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(y=df1[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df1[numeric_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# Create histograms for numeric columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(df1[col], kde=True, bins=30)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make a copy of Price for transformations\n",
    "original_price = df1['Price'].copy()\n",
    "\n",
    "# Apply Box-Cox transformation (fixed: handle non-positive values)\n",
    "# Ensure all prices are positive for Box-Cox transformation\n",
    "min_price = df1['Price'].min()\n",
    "if min_price <= 0:\n",
    "    df1['Price_BoxCox_Input'] = df1['Price'] + abs(min_price) + 1\n",
    "else:\n",
    "    df1['Price_BoxCox_Input'] = df1['Price']\n",
    "\n",
    "# Apply Box-Cox transformation\n",
    "df1['Price_BoxCox'], lambda_val = boxcox(df1['Price_BoxCox_Input'])\n",
    "\n",
    "# Apply Log transformation separately for comparison\n",
    "df1['Price_Log'] = np.log1p(df1['Price'])\n",
    "\n",
    "# Visualize transformations\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df1['Price'], bins=30, kde=True, color=\"red\", edgecolor=\"black\")\n",
    "plt.title(\"Original Price Distribution\")\n",
    "plt.xlabel(\"Price\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df1['Price_Log'], bins=30, kde=True, color=\"green\", edgecolor=\"black\")\n",
    "plt.title(\"Log-Transformed Price Distribution\")\n",
    "plt.xlabel(\"Price_Log\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df1['Price_BoxCox'], bins=30, kde=True, color=\"blue\", edgecolor=\"black\")\n",
    "plt.title(\"Box-Cox Transformed Price Distribution\")\n",
    "plt.xlabel(\"Price_BoxCox\")\n",
    "plt.show()\n",
    "\n",
    "# Choose the best transformation (Box-Cox seems better)\n",
    "df1['Price'] = df1['Price_BoxCox']\n",
    "df1.drop(['Price_BoxCox', 'Price_Log', 'Price_BoxCox_Input'], axis=1, inplace=True)\n",
    "\n",
    "# Transform skewed numeric features\n",
    "skewed_cols = ['RAM', 'storage', 'Battery_cap', 'primary_rear_camera', 'primary_front_camera']\n",
    "for col in skewed_cols:\n",
    "    if skew(df1[col]) > 0.5:  # Apply if skewness > 0.5\n",
    "        df1[col] = np.log1p(df1[col])\n",
    "\n",
    "# Function to cap outliers using IQR\n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df\n",
    "\n",
    "# Apply to relevant columns\n",
    "outlier_cols = ['Price', 'RAM', 'storage', 'Battery_cap', 'primary_rear_camera', 'primary_front_camera']\n",
    "for col in outlier_cols:\n",
    "    df1 = cap_outliers(df1, col)\n",
    "\n",
    "# Create categorical features\n",
    "df1['RAM_category'] = pd.cut(df1['RAM'],\n",
    "                           bins=[0, np.log1p(4), np.log1p(8), np.log1p(12), np.log1p(24)], \n",
    "                           labels=['low', 'medium', 'high', 'very_high'])\n",
    "df1['RAM_category'] = df1['RAM_category'].map({'low': 0, 'medium': 1, 'high': 2, 'very_high': 3}).fillna(0).astype(int)\n",
    "\n",
    "df1['battery_category'] = pd.cut(df1['Battery_cap'], \n",
    "                               bins=[0, np.log1p(3500), np.log1p(4500), np.log1p(6000)], \n",
    "                               labels=['small', 'medium', 'big'])\n",
    "df1['battery_category'] = df1['battery_category'].map({'small': 0, 'medium': 1, 'big': 2}).fillna(0).astype(int)\n",
    "\n",
    "# Encode binary columns\n",
    "binary_cols = ['has_fast_charging', 'has_fingerprints', 'has_nfc', 'has_5g']\n",
    "for col in binary_cols:\n",
    "    df1[col] = df1[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Check the data types before encoding\n",
    "print(\"\\nData types before encoding:\")\n",
    "print(df1.dtypes)\n",
    "\n",
    "# Encode categorical columns - FIXED: Use get_dummies correctly\n",
    "categorical_cols = ['brand_name', 'OS', 'processor_brand', 'display_types']\n",
    "\n",
    "# First, check if these columns exist in the dataframe\n",
    "for col in categorical_cols: \n",
    "    if col not in df1.columns:\n",
    "        print(f\"Warning: Column {col} not found in the dataframe\")\n",
    "\n",
    "# Properly encode all categorical columns\n",
    "df_encoded = pd.get_dummies(df1, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Verify that all object columns are now encoded\n",
    "print(\"\\nData types after encoding:\")\n",
    "print(df_encoded.dtypes)\n",
    "object_cols = df_encoded.select_dtypes(include=['object']).columns\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"Warning: There are still object columns: {object_cols}\")\n",
    "    # Convert any remaining object columns to numeric if possible\n",
    "    for col in object_cols:\n",
    "        try:\n",
    "            df_encoded[col] = pd.to_numeric(df_encoded[col])\n",
    "        except:\n",
    "            print(f\"Could not convert {col} to numeric, dropping it\")\n",
    "            df_encoded = df_encoded.drop(col, axis=1)\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop('Price', axis=1)\n",
    "y = df_encoded['Price']\n",
    "\n",
    "# Check for any remaining non-numeric columns\n",
    "print(\"\\nFinal check for non-numeric columns:\")\n",
    "non_numeric = X.select_dtypes(exclude=['number']).columns\n",
    "if len(non_numeric) > 0:\n",
    "    print(f\"Non-numeric columns found: {non_numeric}\")\n",
    "    # Drop these columns as a last resort\n",
    "    X = X.drop(non_numeric, axis=1)\n",
    "else:\n",
    "    print(\"All columns are numeric - ready for XGBoost!\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and actual values for evaluation\n",
    "# If using Box-Cox, we need to inverse transform\n",
    "from scipy.special import inv_boxcox\n",
    "y_pred_original = inv_boxcox(y_pred, lambda_val)\n",
    "y_test_original = inv_boxcox(y_test, lambda_val)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Absolute Error: ${mean_absolute_error(y_test_original, y_pred_original):.2f}\")\n",
    "print(f\"Root Mean Squared Error: ${np.sqrt(mean_squared_error(y_test_original, y_pred_original)):.2f}\")\n",
    "print(f\"R-squared: {r2_score(y_test_original, y_pred_original):.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Display the most important feature at the top\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter Tuning (optional)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Uncomment to perform grid search (warning: can be time-consuming)\n",
    "# grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "#                           param_grid=param_grid,\n",
    "#                           cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_original, y_pred_original, alpha=0.5)\n",
    "plt.plot([min(y_test_original), max(y_test_original)], [min(y_test_original), max(y_test_original)], 'r--')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted Smartphone Prices')\n",
    "plt.show() '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae57be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df1.isnull().sum())\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "print(\"Unique brands:\", df1['brand_name'].unique())\n",
    "print(\"Unique OS types:\", df1['OS'].unique())\n",
    "print(\"Unique processor brands:\", df1['processor_brand'].unique())\n",
    "print(\"Unique display types:\", df1['display_types'].unique())\n",
    "\n",
    "\n",
    "print(\"Updated processor brands:\", df1['processor_brand'].unique())\n",
    "\n",
    "# Drop the Name column (unnecessary for prediction)\n",
    "df1.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "# Data visualization - uncomment if needed\n",
    "\"\"\"\n",
    "# Brand distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.countplot(data=df1, x='brand_name', color='blue')\n",
    "plt.title(\"Number of Smartphones by Brand\")\n",
    "plt.xlabel(\"Brand\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=50)\n",
    "plt.show()\n",
    "\n",
    "# Price distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df1['Price'], bins=20, color='skyblue', edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of Phone Prices (INR)')\n",
    "plt.xlabel('Price in INR')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Calculate skewness of numeric columns\n",
    "numeric_cols = df1.select_dtypes(include=['number'])\n",
    "skewness_values = numeric_cols.apply(skew)\n",
    "print(\"Skewness of numeric features:\")\n",
    "print(skewness_values)\n",
    "\n",
    "# Boxplots of numeric columns\n",
    "numeric_cols_for_plots = ['Price', 'RAM', 'storage', 'Battery_cap', 'num_core',\n",
    "                          'primary_rear_camera', 'Num_Rear_Cameras', 'primary_front_camera',\n",
    "                          'num_front_camera', 'display_size(inch)', 'refresh_rate(hz)']\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols_for_plots, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(y=df1[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df1[numeric_cols_for_plots].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# Histograms of numeric columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols_for_plots, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(df1[col], kde=True, bins=30)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Save original price for reference\n",
    "original_price = df1['Price'].copy()\n",
    "\n",
    "# Prepare data for Box-Cox transformation (ensure all values are positive)\n",
    "min_price = df1['Price'].min()\n",
    "if min_price <= 0:\n",
    "    df1['Price_BoxCox_Input'] = df1['Price'] + abs(min_price) + 1\n",
    "else:\n",
    "    df1['Price_BoxCox_Input'] = df1['Price']\n",
    "\n",
    "# Apply Box-Cox transformation and store lambda value\n",
    "df1['Price_BoxCox'], lambda_boxcox = boxcox(df1['Price_BoxCox_Input'])\n",
    "\n",
    "# Apply Log transformation for comparison\n",
    "df1['Price_Log'] = np.log1p(df1['Price'])\n",
    "\n",
    "# Visualize transformations - uncomment if needed\n",
    "\"\"\"\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df1['Price'], bins=30, kde=True, color=\"red\", edgecolor=\"black\")\n",
    "plt.title(\"Original Price Distribution\")\n",
    "plt.xlabel(\"Price\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df1['Price_Log'], bins=30, kde=True, color=\"green\", edgecolor=\"black\")\n",
    "plt.title(\"Log-Transformed Price Distribution\")\n",
    "plt.xlabel(\"Price_Log\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df1['Price_BoxCox'], bins=30, kde=True, color=\"blue\", edgecolor=\"black\")\n",
    "plt.title(\"Box-Cox Transformed Price Distribution\")\n",
    "plt.xlabel(\"Price_BoxCox\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Choose the Box-Cox transformation for price\n",
    "df1['Price_Original'] = df1['Price']  # Save the original price\n",
    "df1['Price'] = df1['Price_BoxCox']  # Replace with transformed price\n",
    "\n",
    "# Transform skewed features using log transformation\n",
    "skewed_cols = ['RAM', 'storage', 'Battery_cap', 'primary_rear_camera', 'primary_front_camera']\n",
    "for col in skewed_cols:\n",
    "    if skew(df1[col]) > 0.5:  # Apply if skewness > 0.5\n",
    "        df1[col] = np.log1p(df1[col])\n",
    "\n",
    "print(\"Skewness after transformation:\")\n",
    "print(df1[skewed_cols].skew())\n",
    "\n",
    "# Function to cap outliers using IQR\n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df\n",
    "\n",
    "# Apply to relevant columns\n",
    "outlier_cols = ['Price', 'RAM', 'storage', 'Battery_cap', 'primary_rear_camera', 'primary_front_camera']\n",
    "for col in outlier_cols:\n",
    "    df1 = cap_outliers(df1, col)\n",
    "\n",
    "# Feature engineering\n",
    "df1['RAM_category'] = pd.cut(df1['RAM'], \n",
    "                            bins=[0, np.log1p(4), np.log1p(8), np.log1p(12), np.log1p(24)], \n",
    "                            labels=['low', 'medium', 'high', 'very_high'])\n",
    "\n",
    "df1['RAM_category'] = df1['RAM_category'].map({'low': 0, 'medium': 1, 'high': 2, 'very_high': 3}).fillna(0).astype(int)\n",
    "\n",
    "df1['camera_quality'] = df1['primary_rear_camera'] * df1['Num_Rear_Cameras'].astype(float)\n",
    "\n",
    "df1['battery_category'] = pd.cut(df1['Battery_cap'], \n",
    "                                bins=[0, 3500, 4500, 6000], \n",
    "                                labels=['small', 'medium', 'big'])\n",
    "\n",
    "df1['battery_category'] = df1['battery_category'].map({'small': 0, 'medium': 1, 'big': 2}).fillna(0).astype(int)\n",
    "\n",
    "# Convert binary columns to numeric\n",
    "binary_cols = ['has_fast_charging', 'has_fingerprints', 'has_nfc', 'has_5g']\n",
    "for col in binary_cols:\n",
    "    df1[col] = df1[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = ['brand_name', 'OS', 'processor_brand', 'display_types']\n",
    "df_encoded = pd.get_dummies(df1, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Check for any remaining object columns\n",
    "object_cols = df_encoded.select_dtypes(include=['object']).columns\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"Warning: There are still object columns: {object_cols}\")\n",
    "    # Convert any remaining object columns to numeric if possible\n",
    "    for col in object_cols:\n",
    "        try:\n",
    "            df_encoded[col] = pd.to_numeric(df_encoded[col])\n",
    "        except:\n",
    "            print(f\"Could not convert {col} to numeric, dropping it\")\n",
    "            df_encoded = df_encoded.drop(col, axis=1)\n",
    "\n",
    "# Clean up temporary transformation columns\n",
    "df_encoded.drop(['Price_BoxCox_Input', 'Price_Log'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_encoded.drop(['Price', 'Price_Original'], axis=1, errors='ignore')\n",
    "y = df_encoded['Price']  # Using the transformed price\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check for any remaining non-numeric columns\n",
    "non_numeric = X_train.select_dtypes(exclude=['number']).columns\n",
    "if len(non_numeric) > 0:\n",
    "    print(f\"Non-numeric columns found: {non_numeric}\")\n",
    "    X_train = X_train.drop(non_numeric, axis=1)\n",
    "    X_test = X_test.drop(non_numeric, axis=1)\n",
    "else:\n",
    "    print(\"All columns are numeric - ready for XGBoost!\")\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set (still in transformed space)\n",
    "y_pred_transformed = xgb_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred_original = inv_boxcox(y_pred_transformed, lambda_boxcox)\n",
    "\n",
    "# Get original scale of test data for evaluation\n",
    "y_test_original = df_encoded.loc[y_test.index, 'Price_Original']\n",
    "\n",
    "# Model evaluation\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Absolute Error: INR {mean_absolute_error(y_test_original, y_pred_original):.2f}\")\n",
    "print(f\"Root Mean Squared Error: INR {np.sqrt(mean_squared_error(y_test_original, y_pred_original)):.2f}\")\n",
    "print(f\"R-squared: {r2_score(y_test_original, y_pred_original):.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Display the most important feature at the top\n",
    "plt.show()\n",
    "\n",
    "# Optional: Cross-validation for more robust evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"\\nPerforming 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "print(f\"Cross-validation RMSE scores: {rmse_scores}\")\n",
    "print(f\"Mean RMSE: {rmse_scores.mean():.4f}, Std Dev: {rmse_scores.std():.4f}\")\n",
    "\n",
    "# Function to make predictions for new smartphones\n",
    "def predict_smartphone_price(model, features_dict, lambda_val):\n",
    "    # Convert features to DataFrame\n",
    "    features_df = pd.DataFrame([features_dict])\n",
    "    \n",
    "    # Apply same transformations as training data\n",
    "    for col in skewed_cols:\n",
    "        if col in features_df.columns and features_df[col].iloc[0] > 0:\n",
    "            features_df[col] = np.log1p(features_df[col])\n",
    "    \n",
    "    # Create dummy variables for categorical features\n",
    "    for col in categorical_cols:\n",
    "        if col in features_df.columns:\n",
    "            features_df = pd.get_dummies(features_df, columns=[col], drop_first=True)\n",
    "    \n",
    "    # Align features with training data\n",
    "    missing_cols = set(X.columns) - set(features_df.columns)\n",
    "    for col in missing_cols:\n",
    "        features_df[col] = 0\n",
    "    \n",
    "    # Ensure columns are in the same order\n",
    "    features_df = features_df[X.columns]\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_transformed = model.predict(features_df)[0]\n",
    "    \n",
    "    # Convert back to original scale\n",
    "    pred_original = inv_boxcox(pred_transformed, lambda_val)\n",
    "    \n",
    "    return pred_original\n",
    "\n",
    "# Example usage (uncomment to test)\n",
    "\"\"\"\n",
    "sample_phone = {\n",
    "    'RAM': 8,\n",
    "    'storage': 128,\n",
    "    'Battery_cap': 5000,\n",
    "    'has_fast_charging': 1,\n",
    "    'has_fingerprints': 1,\n",
    "    'has_nfc': 1,\n",
    "    'has_5g': 1,\n",
    "    'num_core': 8,\n",
    "    'primary_rear_camera': 64,\n",
    "    'Num_Rear_Cameras': 4,\n",
    "    'primary_front_camera': 16,\n",
    "    'num_front_camera': 1,\n",
    "    'display_size(inch)': 6.5,\n",
    "    'refresh_rate(hz)': 120,\n",
    "    'brand_name': 'samsung',\n",
    "    'OS': 'android',\n",
    "    'processor_brand': 'snapdragon',\n",
    "    'display_types': 'amoled display'\n",
    "}\n",
    "\n",
    "predicted_price = predict_smartphone_price(xgb_model, sample_phone, lambda_boxcox)\n",
    "print(f\"\\nPredicted price for the sample smartphone: INR {predicted_price:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply to relevant columns\n",
    "outlier_cols = ['Price_Transformed', 'RAM', 'storage', 'Battery_cap', 'primary_rear_camera', 'primary_front_camera']\n",
    "for col in outlier_cols:\n",
    "    df1 = cap_outliers(df1, col)\n",
    "\n",
    "# Feature engineering\n",
    "# Create RAM category\n",
    "df1['RAM_category'] = pd.cut(df1['RAM'], \n",
    "                           bins=[0, np.log1p(4), np.log1p(8), np.log1p(12), np.log1p(24)], \n",
    "                           labels=['low', 'medium', 'high', 'very_high'])\n",
    "df1['RAM_category'] = df1['RAM_category'].map({\n",
    "    'low': 0, 'medium': 1, 'high': 2, 'very_high': 3\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "# Create camera quality feature\n",
    "df1['camera_quality'] = df1['primary_rear_camera'] * df1['Num_Rear_Cameras'].astype(float)\n",
    "\n",
    "# Create battery category - FIXED: Use transformed values \n",
    "df1['battery_category'] = pd.cut(df1['Battery_cap'], \n",
    "                               bins=[0, np.log1p(3500), np.log1p(4500), np.log1p(6000)], \n",
    "                               labels=['small', 'medium', 'big'])\n",
    "df1['battery_category'] = df1['battery_category'].map({\n",
    "    'small': 0, 'medium': 1, 'big': 2\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "# Encode binary columns\n",
    "binary_cols = ['has_fast_charging', 'has_fingerprints', 'has_nfc', 'has_5g']\n",
    "for col in binary_cols:\n",
    "    df1[col] = df1[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# FIXED: Remove the 'Name' column early since it's not useful for modeling\n",
    "df1 = df1.drop('Name', axis=1, errors='ignore')\n",
    "\n",
    "# Check data types before encoding\n",
    "print(\"\\nData types before encoding:\")\n",
    "print(df1.dtypes)\n",
    "\n",
    "# Encode categorical columns - Do this ONCE\n",
    "categorical_cols = ['brand_name', 'OS', 'processor_brand', 'display_types']\n",
    "\n",
    "# Check if these columns exist\n",
    "for col in categorical_cols: \n",
    "    if col not in df1.columns:\n",
    "        print(f\"Warning: Column {col} not found in the dataframe\")\n",
    "\n",
    "# Encode categorical columns\n",
    "df_encoded = pd.get_dummies(df1, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Keep track of original price\n",
    "df_encoded['Original_Price'] = original_price\n",
    "\n",
    "# Clean up price columns - remove intermediate ones\n",
    "df_encoded = df_encoded.drop(['Price', 'Price_BoxCox', 'Price_Log', 'Price_BoxCox_Input'], axis=1, errors='ignore')\n",
    "\n",
    "# Check for any remaining object columns\n",
    "print(\"\\nData types after encoding:\")\n",
    "print(df_encoded.dtypes)\n",
    "\n",
    "object_cols = df_encoded.select_dtypes(include=['object']).columns\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"Warning: There are still object columns: {object_cols}\")\n",
    "    for col in object_cols:\n",
    "        try:\n",
    "            df_encoded[col] = pd.to_numeric(df_encoded[col])\n",
    "        except:\n",
    "            print(f\"Could not convert {col} to numeric, dropping it\")\n",
    "            df_encoded = df_encoded.drop(col, axis=1)\n",
    "\n",
    "# Final check for object columns\n",
    "object_cols = df_encoded.select_dtypes(include=['object']).columns\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"Still have object columns after conversion: {object_cols}\")\n",
    "    df_encoded = df_encoded.drop(object_cols, axis=1)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_encoded.drop(['Price_Transformed', 'Original_Price'], axis=1, errors='ignore')\n",
    "y = df_encoded['Price_Transformed']  # Use transformed price for modeling\n",
    "\n",
    "# Final check for non-numeric columns\n",
    "print(\"\\nFinal check for non-numeric columns:\")\n",
    "non_numeric = X.select_dtypes(exclude=['number']).columns\n",
    "if len(non_numeric) > 0:\n",
    "    print(f\"Non-numeric columns found: {non_numeric}\")\n",
    "    X = X.drop(non_numeric, axis=1)\n",
    "else:\n",
    "    print(\"All columns are numeric - ready for XGBoost!\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "_, _, _, original_y_test = train_test_split(X, df_encoded['Original_Price'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_transformed = xgb_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "from scipy.special import inv_boxcox\n",
    "y_pred_original = inv_boxcox(y_pred_transformed, lambda_val)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Absolute Error: ${mean_absolute_error(original_y_test, y_pred_original):.2f}\")\n",
    "print(f\"Root Mean Squared Error: ${np.sqrt(mean_squared_error(original_y_test, y_pred_original)):.2f}\")\n",
    "print(f\"R-squared: {r2_score(original_y_test, y_pred_original):.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Display the most important feature at the top\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(original_y_test, y_pred_original, alpha=0.5)\n",
    "plt.plot([min(original_y_test), max(original_y_test)], [min(original_y_test), max(original_y_test)], 'r--')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted Smartphone Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to relevant columns (FIX: Use 'Price' instead of 'Price_Transformed')\n",
    "outlier_cols = ['Price', 'RAM', 'storage', 'Battery_cap', 'primary_rear_camera', 'primary_front_camera']\n",
    "for col in outlier_cols:\n",
    "    df1 = cap_outliers(df1, col)\n",
    "\n",
    "# Feature engineering\n",
    "# Create RAM category\n",
    "df1['RAM_category'] = pd.cut(df1['RAM'],\n",
    "                             bins=[-float('inf'), np.log1p(4), np.log1p(8), np.log1p(12), float('inf')],\n",
    "                             labels=['low', 'medium', 'high', 'very_high'])\n",
    "df1['RAM_category'] = df1['RAM_category'].map({\n",
    "    'low': 0, 'medium': 1, 'high': 2, 'very_high': 3\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "# Create camera quality feature\n",
    "df1['camera_quality'] = df1['primary_rear_camera'] * df1['Num_Rear_Cameras'].astype(float)\n",
    "\n",
    "# Create battery category (FIX: Extend bins to include high-capacity batteries)\n",
    "df1['battery_category'] = pd.cut(df1['Battery_cap'],\n",
    "                                bins=[-float('inf'), np.log1p(3500), np.log1p(4500), np.log1p(6000), float('inf')],\n",
    "                                labels=['small', 'medium', 'big', 'very_big'])\n",
    "df1['battery_category'] = df1['battery_category'].map({\n",
    "    'small': 0, 'medium': 1, 'big': 2, 'very_big': 3\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "# Encode binary columns\n",
    "binary_cols = ['has_fast_charging', 'has_fingerprints', 'has_nfc', 'has_5g']\n",
    "for col in binary_cols:\n",
    "    df1[col] = df1[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Remove 'Name' column early\n",
    "df1 = df1.drop('Name', axis=1, errors='ignore')\n",
    "\n",
    "# Rename transformed price for clarity\n",
    "df1['Price_Transformed'] = df1['Price']\n",
    "df1['Original_Price'] = original_price\n",
    "df1 = df1.drop('Price', axis=1)\n",
    "\n",
    "# Check data types before encoding\n",
    "print(\"\\nData types before encoding:\")\n",
    "print(df1.dtypes)\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = ['brand_name', 'OS', 'processor_brand', 'display_types']\n",
    "for col in categorical_cols:\n",
    "    if col not in df1.columns:\n",
    "        print(f\"Warning: Column {col} not found in the dataframe\")\n",
    "\n",
    "df_encoded = pd.get_dummies(df1, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Check for any remaining object columns\n",
    "print(\"\\nData types after encoding:\")\n",
    "print(df_encoded.dtypes)\n",
    "non_numeric = df_encoded.select_dtypes(exclude=['number']).columns\n",
    "if len(non_numeric) > 0:\n",
    "    print(f\"Non-numeric columns found: {non_numeric}\")\n",
    "    df_encoded = df_encoded.drop(non_numeric, axis=1)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nRemaining missing values:\", df_encoded.isna().sum().sum())\n",
    "df_encoded = df_encoded.dropna()  # Drop any remaining NaN rows\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_encoded.drop(['Price_Transformed', 'Original_Price'], axis=1)\n",
    "y = df_encoded['Price_Transformed']\n",
    "\n",
    "# Final check for non-numeric columns\n",
    "print(\"\\nFinal check for non-numeric columns:\")\n",
    "non_numeric = X.select_dtypes(exclude=['number']).columns\n",
    "if len(non_numeric) > 0:\n",
    "    print(f\"Non-numeric columns found: {non_numeric}\")\n",
    "    X = X.drop(non_numeric, axis=1)\n",
    "else:\n",
    "    print(\"All columns are numeric - ready for XGBoost!\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "original_y_test = df_encoded['Original_Price'].loc[y_test.index]  # Use indices for alignment\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "# Train XGBoost model with tuned hyperparameters\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_transformed = xgb_model.predict(X_train)\n",
    "y_pred_test_transformed = xgb_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred_train_original = inv_boxcox(y_pred_train_transformed, lambda_val)\n",
    "y_pred_test_original = inv_boxcox(y_pred_test_transformed, lambda_val)\n",
    "y_train_original = inv_boxcox(y_train, lambda_val)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Training Set:\")\n",
    "print(f\"MAE: ${mean_absolute_error(y_train_original, y_pred_train_original):.2f}\")\n",
    "print(f\"RMSE: ${np.sqrt(mean_squared_error(y_train_original, y_pred_train_original)):.2f}\")\n",
    "print(f\"R²: {r2_score(y_train_original, y_pred_train_original):.4f}\")\n",
    "print(\"Testing Set:\")\n",
    "print(f\"MAE: ${mean_absolute_error(original_y_test, y_pred_test_original):.2f}\")\n",
    "print(f\"RMSE: ${np.sqrt(mean_squared_error(original_y_test, y_pred_test_original)):.2f}\")\n",
    "print(f\"R²: {r2_score(original_y_test, y_pred_test_original):.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(xgb_model, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(-cv_scores)\n",
    "print(f\"\\nCross-Validation RMSE (transformed scale): {cv_rmse.mean():.4f} (+/- {cv_rmse.std() * 2:.4f})\")\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('importance', ascending=False).head(20)\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(original_y_test, y_pred_test_original, alpha=0.5)\n",
    "plt.plot([min(original_y_test), max(original_y_test)], [min(original_y_test), max(original_y_test)], 'r--')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted Smartphone Prices')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
